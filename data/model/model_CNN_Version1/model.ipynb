{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras import datasets, layers, models\n",
        "from keras.utils import np_utils\n",
        "from keras import regularizers\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "g-5UdxgM5ekj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yrExRvYIrWhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lecture des données ......... à partir des ensembles de données Keras et définition des données d'entraînement et de test"
      ],
      "metadata": {
        "id": "UbzG0nXva2NK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()"
      ],
      "metadata": {
        "id": "3speUnTq5eoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploitation des analyses des données: EDA (Exploratory Data Analysis)"
      ],
      "metadata": {
        "id": "G2OUwCa9bEd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vérification du nombre de lignes (records) et de colonnes (features)\n",
        "print(\"train_images: \", train_images.shape)\n",
        "print(\"train_labels: \",train_labels.shape)\n",
        "print(\"test_images: \",test_images.shape)\n",
        "print(\"test_labels: \", test_labels.shape)"
      ],
      "metadata": {
        "id": "Loxhf86ZbGtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vérification du nombre de classes uniques\n",
        "print(\"train_labels\", np.unique(train_labels))\n",
        "print(\"test_labels\", np.unique(test_labels))"
      ],
      "metadata": {
        "id": "gxFcYPn8bNm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Création d'une liste de toutes les étiquettes de classe\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "metadata": {
        "id": "ZAhD84AWbQWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names"
      ],
      "metadata": {
        "id": "uJhXhBpCbSsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualiser certaines des images de l'ensemble de données d'entraînement\n",
        "\n",
        "plt.figure(figsize=[10,10])\n",
        "for i in range (25):    # pour les 25 premières  images\n",
        "  plt.subplot(5, 5, i+1)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.grid(False)\n",
        "  plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "  plt.xlabel(class_names[train_labels[i][0]])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FGyS1fmqbToG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prétraitement des données ( Data Preprocessing ) \n",
        "* La raison de la standardisation/normalisation est de convertir toutes les valeurs de pixel en valeurs comprises entre 0 et 1.\n",
        "* La raison de la conversion de type en float est que to_categorical (hot encodage) a besoin que les données soient de type float par défaut.\n",
        "* La raison de l'utilisation de to_categorical est que la fonction de perte que nous utiliserons dans ce code (categorical_crossentropy) lors de la compilation du modèle a besoin que les données soient codées à chaud.\n"
      ],
      "metadata": {
        "id": "RjwPZBX0bbWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conversion des données de pixels en type flottant\n",
        "train_images = train_images.astype('float32')\n",
        "test_images = test_images.astype('float32')\n",
        " \n",
        "# Standardisation (255 est le nombre total de pixels qu'une image peut avoir)\n",
        "train_images = train_images / 255\n",
        "test_images = test_images / 255 \n",
        "\n",
        "# Encodage  de la classe cible (étiquettes)\n",
        "num_classes = 10\n",
        "train_labels = np_utils.to_categorical(train_labels, num_classes)\n",
        "test_labels = np_utils.to_categorical(test_labels, num_classes)"
      ],
      "metadata": {
        "id": "uRss_U1cbc31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construire le modèle CNN à l'aide de Keras4\n",
        "### Mise en place des couches"
      ],
      "metadata": {
        "id": "b8Yhn_xCbjVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Créer un modèle séquentiel et y ajouter des couches\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(32,32,3)))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Conv2D(32, (3,3), padding='same', activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(layers.Dropout(0.3))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(layers.Dropout(0.5))\n",
        "\n",
        "model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(layers.Dropout(0.5))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(num_classes, activation='softmax'))    # num_classes = 10\n",
        "\n",
        "# Vérification du résumé du modèle (summary)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "jBZFC-4BbgCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compilation du modèle\n",
        "* Optimiseur utilisé lors de la rétropropagation pour l'ajustement du poids et du biais - Adam (ajuste le taux d'apprentissage de manière adaptative).\n",
        "* Fonction de perte utilisée - Entropie croisée catégorique (utilisée lorsque plusieurs catégories/classes sont présentes).\n",
        "* Métriques utilisées pour l'évaluation - Précision."
      ],
      "metadata": {
        "id": "5i8Epj4Wb77-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "z5_M8_5ob3em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ajustement du modèle\n",
        "* La taille du lot est utilisée pour l'optimiseur Adam.\n",
        "* Epoch: Époques - Une époque est un cycle complet (passe avant + passe arrière)"
      ],
      "metadata": {
        "id": "J8rlxQuWcFRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_images, train_labels, batch_size=64, epochs=450,\n",
        "                    validation_data=(test_images, test_labels))"
      ],
      "metadata": {
        "id": "tb4vZAKwcGT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualiser l'évaluation\n",
        "* Courbe de perte (Loss Curve) - Comparaison de la perte d'entraînement avec la perte de test sur des époques croissantes.\n",
        "* Courbe de précision ( Accuracy Curve ) - Comparaison de la précision de l'entraînement avec la précision des tests sur des époques croissantes."
      ],
      "metadata": {
        "id": "NjZYLOKocOVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss curve\n",
        "plt.figure(figsize=[6,4])\n",
        "plt.plot(history.history['loss'], 'black', linewidth=2.0)\n",
        "plt.plot(history.history['val_loss'], 'green', linewidth=2.0)\n",
        "plt.legend(['Training Loss', 'Validation Loss'], fontsize=14)\n",
        "plt.xlabel('Epochs', fontsize=10)\n",
        "plt.ylabel('Loss', fontsize=10)\n",
        "plt.title('Loss Curves', fontsize=12)"
      ],
      "metadata": {
        "id": "1T6PmkeucN_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy curve\n",
        "plt.figure(figsize=[6,4])\n",
        "plt.plot(history.history['accuracy'], 'black', linewidth=2.0)\n",
        "plt.plot(history.history['val_accuracy'], 'blue', linewidth=2.0)\n",
        "plt.legend(['Training Accuracy', 'Validation Accuracy'], fontsize=14)\n",
        "plt.xlabel('Epochs', fontsize=10)\n",
        "plt.ylabel('Accuracy', fontsize=10)\n",
        "plt.title('Accuracy Curves', fontsize=12)"
      ],
      "metadata": {
        "id": "61hLgZh1cYON"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}